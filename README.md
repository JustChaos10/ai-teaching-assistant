# ğŸ§  AI Teaching Assistant

An **AI-powered teaching assistant** with interactive games, a 3D voice chatbot, image detection modules, and teacher-student interfaces â€” all built in Python and JavaScript.  
The goal is to make learning more engaging through **gamification** and **AI-driven interaction**.

---

## âœ¨ Features

- ğŸ® **Gamified Learning**
  - Finger counting and gesture games  
  - Fruits vs. vegetables classifier  
  - Healthy vs. junk food detection  
  - Image-based puzzle activities  

- ğŸ¤– **Chatbot & Avatar**
  - AI chatbot that listens and speaks  
  - Real-time lip-sync animation using **Rhubarb Lip Sync**  
  - Natural TTS (Text-to-Speech) responses  

- ğŸ‘©â€ğŸ« **Teacher Interface**
  - Manage teaching prompts and sessions  
  - Launch activities directly from the GUI  

- ğŸ–¼ï¸ **Image Detection**
  - Computer visionâ€“based teaching modules  
  - Uses OpenCV for educational detection games  

- ğŸ§© **Modular Architecture**
  - Independent modules for Chatbot, Games, and CV tasks  
  - Extensible for adding new teaching tools  

---

## ğŸ“‚ Project Structure

```
ai-teaching-assistant/
â”œâ”€â”€ backend/                         # ğŸ§  FastAPI backend (STT â†’ LLM â†’ TTS â†’ Rhubarb)
â”‚   â”œâ”€â”€ app.py                        # FastAPI entry point
â”‚   â”œâ”€â”€ teacher_chatbot_app.py        # Chatbot pipeline integration
â”‚   â”œâ”€â”€ teacher_chatbot.py            # Core chatbot class
â”‚   â”œâ”€â”€ rag_system.py                 # RAG (Retrieval-Augmented Generation)
â”‚   â”œâ”€â”€ docs/                         # Teaching PDFs for RAG ingestion
â”‚   â”œâ”€â”€ outputs/                      # Generated TTS audio + lip sync JSONs
â”‚   â”œâ”€â”€ templates/test.html           # Upload test UI
â”‚   â””â”€â”€ requirements.txt              # Backend dependencies
â”‚
â”œâ”€â”€ humanoid/                         # ğŸ§ React + Three.js avatar frontend
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â”œâ”€â”€ animations/               # FBX animations (Idle, Greeting, etc.)
â”‚   â”‚   â”œâ”€â”€ models/                   # Avatar GLB model
â”‚   â”‚   â”œâ”€â”€ audios/                   # Audio samples
â”‚   â”‚   â””â”€â”€ textures/                 # Background textures
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ Avatar.jsx            # Lip-sync & TTS logic
â”‚   â”‚   â”‚   â””â”€â”€ Experience.jsx        # Scene setup (lighting, environment)
â”‚   â”‚   â”œâ”€â”€ App.jsx                   # Main React app
â”‚   â”‚   â””â”€â”€ main.jsx                  # Vite entry point
â”‚   â”œâ”€â”€ vite.config.js                # Frontend config
â”‚   â””â”€â”€ package.json                  # Frontend dependencies
â”‚
â”œâ”€â”€ image detector/                   # ğŸ–¼ï¸ CV-based learning modules
â”‚   â”œâ”€â”€ detector.py                   # Object detection logic
â”‚   â”œâ”€â”€ finger_counting_game.py       # Hand gesture recognition
â”‚   â”œâ”€â”€ fruits_vs_vegetables.py       # Food classification
â”‚   â”œâ”€â”€ healthyVSjunk.py              # Food health categorization
â”‚   â”œâ”€â”€ puzzle.py                     # Image puzzle mini-game
â”‚   â”œâ”€â”€ images/, temp_images/         # Game assets
â”‚   â”œâ”€â”€ puzzle_sprites/               # Sprite resources
â”‚   â””â”€â”€ requirements.txt              # CV dependencies
â”‚
â”œâ”€â”€ asset/                            # âš™ï¸ (LFS) AI model weights (ignored in Git)
â”‚   â”œâ”€â”€ DVAE.safetensors
â”‚   â”œâ”€â”€ Decoder.safetensors
â”‚   â”œâ”€â”€ Embed.safetensors
â”‚   â”œâ”€â”€ Vocos.safetensors
â”‚   â””â”€â”€ gpt/model.safetensors
â”‚
â”œâ”€â”€ animations/                       # ğŸŒ€ GIF animations (idle/speaking/thinking)
â”‚   â”œâ”€â”€ idle.gif
â”‚   â”œâ”€â”€ speaking.gif
â”‚   â””â”€â”€ listening.gif
â”‚
â”œâ”€â”€ requirements.txt                  # Master dependency list
â”œâ”€â”€ requirements_updated.txt          # Full merged dependency list
â”‚
â”œâ”€â”€ launch_teacher_interface.py       # Launch complete teaching interface
â”œâ”€â”€ teacher_interface.py              # Teacher GUI
â”œâ”€â”€ module_executor.py                # Module manager (games/chatbot/CV)
â”œâ”€â”€ chatbot_logic.py                  # Dialogue management
â”œâ”€â”€ setup_api_keys.py                 # Environment variable setup
â”œâ”€â”€ quick_test.py                     # Quick STT â†’ LLM â†’ TTS test
â”‚
â””â”€â”€ LICENSE                           # MIT license
```

---
## âš™ï¸ Prerequisites

Make sure you have:

- [Python 3.10+](https://www.python.org/downloads/)
- [Node.js 18+](https://nodejs.org/en/download/)
- [Git](https://git-scm.com/downloads/)
- [Rhubarb Lip Sync](https://github.com/DanielSWolf/rhubarb-lip-sync/releases)

> ğŸ’¡ On Windows, install Rhubarb to:
> ```
> C:\tools\rhubarb\rhubarb.exe
> ```

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/JustChaos10/ai-teaching-assistant.git
cd ai-teaching-assistant
```

---

### 2ï¸âƒ£ Create a Virtual Environment
```bash
python3.10 -m venv venv310
# On Linux/Mac
source venv310/bin/activate
# On Windows
venv310\Scripts\activate
```

---

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements_updated.txt
```

---

## ğŸ§© Backend Setup (FastAPI)

1. Navigate to the backend folder:
   ```bash
   cd Capstone/backend
   ```

2. Run the FastAPI server:
   ```bash
   uvicorn app:app --reload --port 8000
   ```

3. Wait for:
   ```bash
   Application startup complete
   ```
   âœ… Now your backend runs on:
   ```bash
   http://127.0.0.1:8000
   ```

---

## ğŸ’» Frontend Setup (React + React Three Fiber)

1. Open a **new terminal (make sure venv310 is activated again in this new terminal)**:
   ```bash
   cd Capstone/humanoid
   ```

2. Install Node.js:
   - Download from [Node.js LTS](https://nodejs.org/en/download/)
   - Check â€œAdd to PATHâ€ during setup.

3. Verify installation:
   ```bash
   node -v
   npm -v
   ```

4. Install frontend dependencies:
   ```bash
   npm install
   ```

5. Run the frontend:
   ```bash
   npm run dev
   ```

6. Open your browser at:
   ```bash
   http://localhost:5173/
   ```

---



---

## ğŸ” Environment Variables

Create a `.env` file in the project root:
```bash
OPENAI_API_KEY=your_openai_api_key
MURF_API_KEY=your_murf_api_key
```

---

## ğŸ§  How It Works

```
ğŸ™ï¸ Voice Input
   â†“
ğŸ§  FastAPI backend (Whisper â†’ LLM â†’ TTS)
   â†“
ğŸ§ Audio + Rhubarb JSON
   â†“
ğŸ§ React Avatar (mouth animation syncs with phonemes)
```

---

## ğŸ§° Commands

| Task | Command |
|------|----------|
| Run backend | `uvicorn app:app --reload` |
| Run frontend | `npm run dev` |
| Install backend deps | `pip install -r requirements.txt` |
| Install frontend deps | `npm install` |
| Build frontend | `npm run build` |
| Clean npm cache | `npm cache clean --force` |

---

## ğŸ§© Tech Stack

| Component | Technology |
|------------|-------------|
| Voice Recognition | OpenAI Whisper |
| Text-to-Speech | Murf.ai |
| Lip Sync | Rhubarb Lip Sync |
| 3D Rendering | React Three Fiber |
| Backend | FastAPI |
| Frontend | React + Vite |
| Avatar Model | Ready Player Me |
| Animations | Mixamo FBX |

---

## ğŸ“¦ Deployment Notes

- Do **not** commit large `.safetensors` model files to GitHub.
- Add to `.gitignore`:
  ```
  venv310/
  __pycache__/
  outputs/
  node_modules/
  asset/
  *.safetensors
  ```

You can host:
- **Backend:** Render / Railway / AWS EC2  
- **Frontend:** Vercel / Netlify

---



---

## ğŸ“œ License

This project is licensed under the **MIT License**.  
See [LICENSE](LICENSE) for details.

---

> ğŸŸ¢ *â€œMaking education more interactive, one avatar at a time.â€*
